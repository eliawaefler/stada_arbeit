import streamlit as st
import pandas as pd
import os
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Dateien einlesen
mobility_files = [
    "zurich_mobility_1.csv",
    "zurich_mobility_2.csv",
    "zurich_mobility_3.csv"
]

mobility_dfs = []
for file in mobility_files:
    if os.path.exists(file):
        df = pd.read_csv(file)
        mobility_dfs.append(df)
    else:
        st.warning(f"Datei nicht gefunden: {file}")

# Mobility-Daten zusammenf√ºhren
if mobility_dfs:
    mobility_df = pd.concat(mobility_dfs, ignore_index=True)
else:
    mobility_df = pd.DataFrame()

# Wetter- und Standortdaten laden
try:
    wetter_df = pd.read_csv("zurich_wetter.csv")
except FileNotFoundError:
    wetter_df = pd.DataFrame()
    st.warning("zurich_wetter.csv nicht gefunden.")

try:
    standorte_df = pd.read_csv("zurich_standorte.csv")
except FileNotFoundError:
    standorte_df = pd.DataFrame()
    st.warning("zurich_standorte.csv nicht gefunden.")


# Zeitspalten vereinheitlichen
mobility_df["DATUM"] = pd.to_datetime(mobility_df["DATUM"])
wetter_df["dt_iso"] = pd.to_datetime(wetter_df["dt_iso"])
mobility_df["DATUM"] = mobility_df["DATUM"].dt.floor("H")
wetter_df["dt_iso"] = wetter_df["dt_iso"].dt.floor("H")

# Aggregation √ºber alle Standorte
mobility_agg = mobility_df.groupby("DATUM")[["VELO_IN", "VELO_OUT", "FUSS_IN", "FUSS_OUT"]].sum().reset_index()
df = pd.merge(mobility_agg, wetter_df, left_on="DATUM", right_on="dt_iso", how="inner")


# Sidebar Navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio("Seite ausw√§hlen", [
    "Start",
    "Deskriptive Statistik",
    "Multiple Lineare Regression (MLR)",
    "PCA",
    "Zeitreihenanalyse",
    "Clustering (K-Means)"
])

# Startseite
if page == "Start":
    st.title("Statistische Auswertung ‚Äì Z√ºrich Mobility & Wetter")
    st.write("""
    Willkommen!  
    Diese Anwendung zeigt statistische Methoden auf Grundlage von Mobilit√§ts- und Wetterdaten aus Z√ºrich.  
    Links findest du die Methoden:
    - Deskriptive Statistik
    - Multiple Lineare Regression
    - PCA (Hauptkomponentenanalyse)
    - Zeitreihenanalyse
    - Clustering (K-Means)
    """)

    st.subheader("üö≤ Zusammengesetzte Mobility-Daten (Auszug)")
    st.dataframe(mobility_df.head(100))  # Anzeige auf 100 Zeilen begrenzt

    st.subheader("üå¶ Wetterdaten Z√ºrich (Auszug)")
    st.dataframe(wetter_df.head(100))

    st.subheader("üìç Standorte der Z√§hlstationen (Auszug)")
    st.dataframe(standorte_df.head(100))

# Deskriptive Statistik
elif page == "Deskriptive Statistik":
    st.title("Deskriptive Statistik")
    st.write("""
    Beschreibt Daten durch Kennzahlen wie:
    - Mittelwert, Median, Modus
    - Standardabweichung, Varianz, Spannweite
    - Histogramme, Boxplots

    Ziel: **Verst√§ndliche Zusammenfassung der Daten** ohne Modelle.
    """)

    st.subheader("Numerische Grundauswertung")
    numeric_cols = ["VELO_IN", "VELO_OUT", "FUSS_IN", "FUSS_OUT"]
    st.dataframe(mobility_df[numeric_cols].describe())

    st.subheader("Fehlende Werte je Kategorie")
    missing = mobility_df[numeric_cols].isnull().sum().to_frame(name="Anzahl fehlender Werte")
    st.dataframe(missing)

    st.subheader("Verteilung (Histogramm)")

    selected_column = st.selectbox("W√§hle eine Spalte f√ºr das Histogramm", numeric_cols)

    if selected_column:
        st.bar_chart(mobility_df[selected_column].dropna().value_counts().sort_index())

    st.subheader("Korrelation zwischen Bewegungsarten")
    correlation = mobility_df[numeric_cols].corr()
    st.dataframe(correlation)

# MLR
elif page == "Multiple Lineare Regression (MLR)":
    st.title("Multiple Lineare Regression (MLR)")
    st.write("""
    Modelliert den Zusammenhang zwischen einer Zielvariable \( Y \) und mehreren Einflussgr√∂√üen \( X_1, X_2, \dots \):

    \[
    Y = \beta_0 + \beta_1 X_1 + \dots + \beta_n X_n + \epsilon
    \]

    **Residuenanalyse** pr√ºft, ob das Modell passend ist (z.‚ÄØB. Normalverteilung der Fehler, konstante Varianz).
    """)


    st.write("Diese Analyse versucht, Mobilit√§tsverhalten anhand von Wetterdaten vorherzusagen.")

    target = st.selectbox("Zu prognostizierende Zielvariable", ["VELO_IN", "VELO_OUT", "FUSS_IN", "FUSS_OUT"])

    wetter_features = [
        "temp", "humidity", "wind_speed", "clouds_all",
        "dew_point", "feels_like", "pressure", "visibility"
    ]

    # Drop Zeilen mit fehlenden Werten
    df_ml = df[[target] + wetter_features].dropna()

    # Input & Target definieren
    X = df_ml[wetter_features]
    y = df_ml[target]

    # Split in Trainings- und Testdaten
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Modell trainieren
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Vorhersagen
    y_pred = model.predict(X_test)

    st.subheader("Modellg√ºte")
    st.write(f"**R¬≤ Score:** {r2_score(y_test, y_pred):.3f}")
    st.write(f"**RMSE:** {mean_squared_error(y_test, y_pred, squared=False):.2f}")

    st.subheader("Modellkoeffizienten")
    coeff_df = pd.DataFrame({
        "Merkmal": wetter_features,
        "Koeffizient": model.coef_
    })
    st.dataframe(coeff_df)

    st.subheader("Vorhergesagte vs. echte Werte (Streudiagramm)")
    st.scatter_chart(pd.DataFrame({"Echt": y_test, "Vorhersage": y_pred}))

# PCA
elif page == "PCA":
    st.title("PCA ‚Äì Hauptkomponentenanalyse")
    st.write("""
    PCA reduziert die Dimension der Daten durch Transformation in neue Achsen (Hauptkomponenten), die m√∂glichst viel Varianz erhalten.

    Vorteile:
    - Weniger Variablen
    - Bessere Visualisierbarkeit
    - Vorbereitung f√ºr ML oder Clustering
    """)

    st.write("""
        Diese Seite f√ºhrt eine PCA durch, um Mobilit√§ts- und Wettermerkmale auf 2 Dimensionen zu reduzieren.  
        So l√§sst sich z.‚ÄØB. erkennen, ob √§hnliche Tage/Wetterlagen √§hnliche Mobilit√§tsmuster erzeugen.
        """)

    # Auswahl der Features f√ºr PCA
    features = [
        "VELO_IN", "VELO_OUT", "FUSS_IN", "FUSS_OUT",
        "temp", "humidity", "wind_speed", "clouds_all",
        "dew_point", "feels_like", "pressure", "visibility"
    ]

    df_pca = df[features].dropna()

    # Standardisierung
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df_pca)

    # PCA durchf√ºhren
    pca = PCA(n_components=2)
    components = pca.fit_transform(scaled_data)

    # Ergebnis als DataFrame
    pca_df = pd.DataFrame(data=components, columns=["PC1", "PC2"])

    st.subheader("Scatterplot der Hauptkomponenten")
    st.write("Jeder Punkt ist ein Zeitpunkt (eine Stunde).")
    st.scatter_chart(pca_df)

    st.subheader("Erkl√§rte Varianz")
    st.write(f"PC1: {pca.explained_variance_ratio_[0]:.2%}")
    st.write(f"PC2: {pca.explained_variance_ratio_[1]:.2%}")

# Zeitreihenanalyse
elif page == "Zeitreihenanalyse":
    st.title("Zeitreihenanalyse")
    st.write("""
    Analyse von Daten √ºber die Zeit, z.‚ÄØB. Wetter oder Mobilit√§tszahlen je Stunde/Tag.

    Ziel: Muster erkennen (Trend, Saisonalit√§t, Zyklen) und **Vorhersagen** treffen.  
    Typische Modelle: AR, MA, ARIMA
    """)
    st.write("Hier wird der zeitliche Verlauf der Bewegungen in Z√ºrich visualisiert.")

    variable = st.selectbox("W√§hle eine Variable f√ºr den Zeitverlauf", ["VELO_IN", "VELO_OUT", "FUSS_IN", "FUSS_OUT"])

    st.line_chart(mobility_agg.set_index("DATUM")[variable])

    st.subheader("Gleitender Durchschnitt (24h)")
    mobility_agg[f"{variable}_SMA24"] = mobility_agg[variable].rolling(window=24).mean()
    st.line_chart(mobility_agg.set_index("DATUM")[f"{variable}_SMA24"])

    st.subheader("T√§gliche Gesamtsumme")
    daily = mobility_agg.copy()
    daily["DATUM"] = daily["DATUM"].dt.date
    daily_sum = daily.groupby("DATUM")[variable].sum()
    st.line_chart(daily_sum)

# Clustering
elif page == "Clustering (K-Means)":
    st.title("Clustering ‚Äì K-Means")
    st.write("""
    Un√ºberwachtes Verfahren zur Gruppierung √§hnlicher Datenpunkte in **\( k \)** Cluster.

    Schritte:
    1. W√§hle \( k \)
    2. Initialisiere Clusterzentren
    3. Weise Punkte zu, berechne neue Zentren
    4. Wiederhole bis Konvergenz

    Ziel: **√Ñhnliche Daten gruppieren** (z.‚ÄØB. Verkehrsorte mit √§hnlichem Muster)
    """)


    st.write("""
    Hier werden Zeitpunkte (Stunden) anhand von Wetter- und Mobilit√§tsdaten in Cluster gruppiert.  
    Dies kann typische Mobilit√§tsmuster (z.‚ÄØB. ‚ÄûRush Hour bei Regen‚Äú) aufdecken.
    """)

    # Features w√§hlen
    features = [
        "VELO_IN", "VELO_OUT", "FUSS_IN", "FUSS_OUT",
        "temp", "humidity", "wind_speed", "clouds_all",
        "dew_point", "feels_like", "pressure", "visibility"
    ]

    df_cluster = df[features].dropna()

    # Standardisieren
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_cluster)

    # Cluster-Anzahl ausw√§hlen
    k = st.slider("Anzahl Cluster (k)", 2, 10, 4)

    # KMeans Clustering
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)

    # PCA zur Darstellung
    pca = PCA(n_components=2)
    components = pca.fit_transform(X_scaled)
    pca_df = pd.DataFrame(components, columns=["PC1", "PC2"])
    pca_df["Cluster"] = labels.astype(str)

    st.subheader("Visualisierung der Cluster (PCA-Projektion)")
    st.write("Jeder Punkt ist ein Zeitpunkt, die Farben zeigen Clusterzugeh√∂rigkeit.")

    # Interaktiver Scatterplot
    st.scatter_chart(pca_df, x="PC1", y="PC2", color="Cluster")
